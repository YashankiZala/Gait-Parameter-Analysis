{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "pRRWd0-zmGnj",
        "outputId": "43146b12-06e7-4ec7-b465-b4bb7afe6708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping mediapipe as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting mediapipe==0.10.8\n",
            "  Downloading mediapipe-0.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.8) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.8) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.8) (25.2.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.8) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.8) (2.0.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.8) (4.11.0.86)\n",
            "Collecting protobuf<4,>=3.11 (from mediapipe==0.10.8)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.8)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.8) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.8) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.8) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.8) (1.17.0)\n",
            "Downloading mediapipe-0.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.8 protobuf-3.20.3 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "aa45625303634526af80b4b0910355e9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall -y mediapipe\n",
        "!pip install mediapipe==0.10.8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW ONE\n",
        "import select\n",
        "\n",
        "# Patch select to avoid WinError 10038\n",
        "_ORIGINAL_SELECT = select.select\n",
        "def _safe_select(*args, **kwargs):\n",
        "    try:\n",
        "        return _ORIGINAL_SELECT(*args, **kwargs)\n",
        "    except OSError as e:\n",
        "        if e.winerror == 10038:\n",
        "            return [], [], []\n",
        "        raise\n",
        "select.select = _safe_select\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import mediapipe as mp\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(\n",
        "    static_image_mode=False,\n",
        "    model_complexity=2,\n",
        "    min_detection_confidence=0.7,\n",
        "    min_tracking_confidence=0.7\n",
        ")\n",
        "\n",
        "POINTS = {\n",
        "    \"shoulder\": 12,\n",
        "    \"hip\": 24,\n",
        "    \"knee\": 26,\n",
        "    \"ankle\": 28,\n",
        "    \"toe\": 32,\n",
        "    \"left_ankle\": 27,\n",
        "    \"left_toe\": 31\n",
        "}\n",
        "\n",
        "UNIFORM_COLOR = (0, 255, 255)\n",
        "\n",
        "def extract_keypoints(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Unable to open video file.\")\n",
        "        return np.array([]), [], {}, None, 0\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    keypoints, frames = [], []\n",
        "    trajectories = {name: [] for name in POINTS}\n",
        "    pixel_to_meter = None\n",
        "    frame_width, frame_height = 0, 0\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_width == 0:\n",
        "                frame_height, frame_width = frame.shape[:2]\n",
        "\n",
        "            frames.append(frame.copy())\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = pose.process(rgb)\n",
        "\n",
        "            if results.pose_landmarks:\n",
        "                lm = results.pose_landmarks.landmark\n",
        "                kp = np.array([[p.x, p.y] for p in lm])\n",
        "                keypoints.append(kp)\n",
        "\n",
        "                if pixel_to_meter is None:\n",
        "                    hip = lm[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
        "                    ankle = lm[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
        "                    leg_length_px = abs(hip.y - ankle.y) * frame_height\n",
        "\n",
        "                    if leg_length_px > 10:\n",
        "                        pixel_to_meter = (0.45 * 1.7) / leg_length_px\n",
        "                        print(f\"Calibrated: 1px = {pixel_to_meter:.6f}m\")\n",
        "                    else:\n",
        "                        pixel_to_meter = 0.001\n",
        "                        print(\"Using default calibration\")\n",
        "\n",
        "                for name, idx in POINTS.items():\n",
        "                    landmark = lm[idx]\n",
        "                    if landmark.visibility > 0.5:\n",
        "                        x = int(landmark.x * frame_width)\n",
        "                        y = int(landmark.y * frame_height)\n",
        "                        trajectories[name].append((x, y))\n",
        "                    else:\n",
        "                        if trajectories[name]:\n",
        "                            trajectories[name].append(trajectories[name][-1])\n",
        "                        else:\n",
        "                            trajectories[name].append((0, 0))\n",
        "            else:\n",
        "                for name in POINTS:\n",
        "                    if trajectories[name]:\n",
        "                        trajectories[name].append(trajectories[name][-1])\n",
        "                    else:\n",
        "                        trajectories[name].append((0, 0))\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    if pixel_to_meter is None:\n",
        "        print(\"Warning: Using default calibration\")\n",
        "        pixel_to_meter = 0.001\n",
        "\n",
        "    return np.array(keypoints), frames, trajectories, pixel_to_meter, fps\n",
        "\n",
        "def angle(a, b, c):\n",
        "    ab, bc = b - a, c - b\n",
        "    cos_angle = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))\n",
        "    return np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
        "\n",
        "def detect_steps_by_minima(right_ankle_traj, left_ankle_traj, min_prominence=10, min_distance=10):\n",
        "    arr1 = np.array(right_ankle_traj)\n",
        "    arr2 = np.array(left_ankle_traj)\n",
        "    mask = ((arr1 != (0, 0)).all(axis=1)) & ((arr2 != (0, 0)).all(axis=1))\n",
        "    dists = np.full(len(arr1), np.nan)\n",
        "    dists[mask] = np.linalg.norm(arr1[mask] - arr2[mask], axis=1)\n",
        "    peaks, _ = find_peaks(-dists, prominence=min_prominence, distance=min_distance)\n",
        "    return peaks.tolist(), dists\n",
        "\n",
        "def calculate_gait_parameters(keypoints, trajectories, pixel_to_meter, fps):\n",
        "    if keypoints.size == 0 or len(trajectories[\"ankle\"]) < 2:\n",
        "        return np.zeros(6), [], [], []\n",
        "\n",
        "    right_ankle_traj = trajectories[\"ankle\"]\n",
        "    left_ankle_traj = trajectories[\"left_ankle\"]\n",
        "\n",
        "    step_events, dists = detect_steps_by_minima(right_ankle_traj, left_ankle_traj)\n",
        "\n",
        "    stride_lengths = []\n",
        "    for i in range(1, len(step_events)):\n",
        "        idx1, idx2 = step_events[i-1], step_events[i]\n",
        "        pt1 = np.array(right_ankle_traj[idx1])\n",
        "        pt2 = np.array(right_ankle_traj[idx2])\n",
        "        if (pt1 != 0).all() and (pt2 != 0).all():\n",
        "            stride_lengths.append(np.linalg.norm(pt2 - pt1) * pixel_to_meter)\n",
        "    stride_length = np.mean(stride_lengths) if stride_lengths else 0\n",
        "\n",
        "    # Cadence: steps per minute, using time between first and last detected step\n",
        "    if len(step_events) > 1:\n",
        "        total_time_sec = (step_events[-1] - step_events[0]) / fps\n",
        "        cadence = (len(step_events) - 1) / total_time_sec * 60 if total_time_sec > 0 else 0\n",
        "    else:\n",
        "        cadence = 0\n",
        "\n",
        "    speed_kmph = stride_length * cadence / 120 * 3.6 if cadence > 0 else 0\n",
        "\n",
        "    knee_angles, hip_angles, ankle_angles = [], [], []\n",
        "    for i in range(len(keypoints)):\n",
        "        hip = keypoints[i, POINTS[\"hip\"]]\n",
        "        knee = keypoints[i, POINTS[\"knee\"]]\n",
        "        ankle = keypoints[i, POINTS[\"ankle\"]]\n",
        "        toe = keypoints[i, POINTS[\"toe\"]]\n",
        "        shoulder = keypoints[i, POINTS[\"shoulder\"]]\n",
        "        knee_angles.append(angle(hip, knee, ankle))\n",
        "        hip_angles.append(angle(shoulder, hip, knee))\n",
        "        ankle_angles.append(angle(knee, ankle, toe))\n",
        "\n",
        "    return np.array([\n",
        "        stride_length,\n",
        "        cadence,\n",
        "        np.mean(knee_angles),\n",
        "        np.mean(hip_angles),\n",
        "        np.mean(ankle_angles),\n",
        "        speed_kmph\n",
        "    ]), [knee_angles, hip_angles, ankle_angles], step_events\n",
        "\n",
        "def overlay_text_on_video(frames, gait_params, suggestions, joint_angles,\n",
        "                          output_path, trajectories, step_events):\n",
        "    if not frames:\n",
        "        print(\"Error: No frames to process\")\n",
        "        return\n",
        "\n",
        "    height, width = frames[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    step_events_set = set(step_events)\n",
        "\n",
        "    for i, frame in enumerate(frames):\n",
        "        disp = frame.copy()\n",
        "\n",
        "        # Draw trajectories for all joints except left_ankle\n",
        "        for name, points in trajectories.items():\n",
        "            if name in [\"left_ankle\", \"left_toe\"]:\n",
        "                continue\n",
        "            for j in range(1, min(i+1, len(points))):\n",
        "                if points[j-1] != (0,0) and points[j] != (0,0):\n",
        "                    alpha = max(0.3, j/len(points))\n",
        "                    col = tuple(int(c*alpha) for c in UNIFORM_COLOR)\n",
        "                    cv2.line(disp, points[j-1], points[j], col, 2)\n",
        "\n",
        "        # Draw only points at joints (no skeleton lines)\n",
        "        for name, points in trajectories.items():\n",
        "            if i < len(points) and points[i] != (0,0):\n",
        "                x, y = points[i]\n",
        "                cv2.circle(disp, (x, y), 6, UNIFORM_COLOR, -1)\n",
        "                cv2.circle(disp, (x, y), 8, (255, 255, 255), 2)\n",
        "\n",
        "        # Mark step events (ankle collisions)\n",
        "        if i in step_events_set:\n",
        "            right_ankle_pos = trajectories[\"ankle\"][i]\n",
        "            left_ankle_pos = trajectories[\"left_ankle\"][i]\n",
        "            if right_ankle_pos != (0,0):\n",
        "                cv2.circle(disp, right_ankle_pos, 12, (0, 0, 255), 3)\n",
        "            if left_ankle_pos != (0,0):\n",
        "                cv2.circle(disp, left_ankle_pos, 12, (255, 0, 0), 3)\n",
        "\n",
        "        y_offset = 30\n",
        "        if i < len(joint_angles[0]):\n",
        "            knee_angle = joint_angles[0][i]\n",
        "            hip_angle = joint_angles[1][i]\n",
        "            ankle_angle = joint_angles[2][i]\n",
        "        else:\n",
        "            knee_angle = joint_angles[0][-1] if joint_angles[0] else 0\n",
        "            hip_angle = joint_angles[1][-1] if joint_angles[1] else 0\n",
        "            ankle_angle = joint_angles[2][-1] if joint_angles[2] else 0\n",
        "\n",
        "        params_text = [\n",
        "            f\"Stride Length: {gait_params[0]:.4f} m\",\n",
        "            f\"Cadence: {gait_params[1]:.2f} steps/min\",\n",
        "            f\"Hip Angle: {hip_angle:.1f} deg\",\n",
        "            f\"Knee Angle: {knee_angle:.1f} deg\",\n",
        "            f\"Ankle Angle: {ankle_angle:.1f} deg\",\n",
        "            f\"Speed: {gait_params[5]:.2f} km/h\",\n",
        "            f\"Steps: {len(step_events)}\"\n",
        "        ]\n",
        "\n",
        "        for text in params_text:\n",
        "            cv2.putText(disp, text, (20, y_offset), font, 0.6, (0, 0, 0), 3)\n",
        "            cv2.putText(disp, text, (20, y_offset), font, 0.6, (0, 255, 0), 2)\n",
        "            y_offset += 25\n",
        "\n",
        "        for idx, suggestion in enumerate(suggestions):\n",
        "            cv2.putText(disp, suggestion, (20, height - 40 - idx * 25),\n",
        "                        font, 0.6, (0, 0, 0), 3)\n",
        "            cv2.putText(disp, suggestion, (20, height - 40 - idx * 25),\n",
        "                        font, 0.6, (0, 255, 255), 2)\n",
        "\n",
        "        out.write(disp)\n",
        "\n",
        "    out.release()\n",
        "\n",
        "def generate_suggestions(gait_params):\n",
        "    suggestions = []\n",
        "    OPTIMAL_VALUES = {\n",
        "        \"stride_length\": (0.6, 1.0),\n",
        "        \"cadence\": (70, 120),\n",
        "        \"knee_angle\": (10, 70),\n",
        "        \"hip_angle\": (10, 40),\n",
        "        \"ankle_angle\": (10, 30)\n",
        "    }\n",
        "    param_names = [\"Stride Length\", \"Cadence\", \"Knee Angle\", \"Hip Angle\", \"Ankle Angle\"]\n",
        "\n",
        "    for i, (param, (low, high)) in enumerate(OPTIMAL_VALUES.items()):\n",
        "        value = gait_params[i]\n",
        "        if value < low:\n",
        "            suggestions.append(f\"Increase {param_names[i]} to improve gait.\")\n",
        "        elif value > high:\n",
        "            suggestions.append(f\"Decrease {param_names[i]} to improve posture.\")\n",
        "\n",
        "    if not suggestions:\n",
        "        suggestions.append(\"Gait parameters are within optimal range!\")\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "def process_video(video_path, output_path=\"output_video.mp4\"):\n",
        "    try:\n",
        "        MODEL_PATH = \"gait_model.pkl\"\n",
        "        DATA_PATH = \"training_data.pkl\"\n",
        "\n",
        "        if os.path.exists(MODEL_PATH):\n",
        "            model = joblib.load(MODEL_PATH)\n",
        "        else:\n",
        "            model = RandomForestRegressor(warm_start=True)\n",
        "\n",
        "        if os.path.exists(DATA_PATH):\n",
        "            X_train, y_train = joblib.load(DATA_PATH)\n",
        "        else:\n",
        "            X_train, y_train = [], []\n",
        "\n",
        "        keypoints, frames, trajectories, pixel_to_meter, fps = extract_keypoints(video_path)\n",
        "\n",
        "        if keypoints.size == 0 or not frames:\n",
        "            raise Exception(\"No keypoints or frames extracted from video\")\n",
        "\n",
        "        gait_params, joint_angles, step_events = calculate_gait_parameters(\n",
        "            keypoints, trajectories, pixel_to_meter, fps\n",
        "        )\n",
        "\n",
        "        suggestions = generate_suggestions(gait_params)\n",
        "\n",
        "        print(f\"Detected {len(step_events)} steps\")\n",
        "        print(f\"Stride Length: {gait_params[0]:.4f}m\")\n",
        "        print(f\"Cadence: {gait_params[1]:.2f} steps/min\")\n",
        "        print(f\"Speed: {gait_params[5]:.2f} km/h\")\n",
        "\n",
        "        overlay_text_on_video(\n",
        "            frames,\n",
        "            gait_params,\n",
        "            suggestions,\n",
        "            joint_angles,\n",
        "            output_path,\n",
        "            trajectories,\n",
        "            step_events\n",
        "        )\n",
        "\n",
        "        if joint_angles[0]:\n",
        "            knee_mean = np.mean(joint_angles[0])\n",
        "            if not np.isnan(knee_mean):\n",
        "                X_train.append(gait_params.tolist())\n",
        "                y_train.append(knee_mean)\n",
        "                model.fit(np.array(X_train), np.array(y_train))\n",
        "                joblib.dump(model, MODEL_PATH)\n",
        "                joblib.dump((X_train, y_train), DATA_PATH)\n",
        "                print(\"Model updated and saved\")\n",
        "\n",
        "        return gait_params\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJbr9UJCfAV4",
        "outputId": "d993cad0-2981-4063-b95a-64690b6769e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model to /usr/local/lib/python3.11/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8JiSoZZ9rZv",
        "outputId": "2f9788cb-0e4d-4b0e-a915-34001a8c8222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrated: 1px = 0.004288m\n",
            "Detected 9 steps\n",
            "Stride Length: 0.5681m\n",
            "Cadence: 101.52 steps/min\n",
            "Speed: 1.73 km/h\n",
            "Model updated and saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:466: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.56810583, 101.52375815,  10.90124422,   7.35888282,\n",
              "        52.63148944,   1.73028716])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "video_path = \"walk-2.mp4\"\n",
        "process_video(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hGeOw6SvCFkX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05834db5-31c0-4c34-a545-dbea610e2948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d7c68ea9-b172-4787-b837-02683cc38c2f\", \"blurred_faces_output.mp4\", 4007513)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "input_video_path = 'output_video.mp4'\n",
        "output_video_path = 'blurred_faces_output.mp4'\n",
        "\n",
        "if not os.path.isfile(input_video_path):\n",
        "    raise FileNotFoundError(f\"Input video not found at {input_video_path}\")\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False,\n",
        "    max_num_faces=2,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
        "\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results_mesh = face_mesh.process(rgb)\n",
        "\n",
        "    face_found = False\n",
        "    if results_mesh.multi_face_landmarks:\n",
        "        for face_landmarks in results_mesh.multi_face_landmarks:\n",
        "            points = []\n",
        "            for lm in face_landmarks.landmark:\n",
        "                x = int(lm.x * width)\n",
        "                y = int(lm.y * height)\n",
        "                points.append([x, y])\n",
        "            points = np.array(points, dtype=np.int32)\n",
        "            points = points[(points[:,0] >= 0) & (points[:,0] < width) & (points[:,1] >= 0) & (points[:,1] < height)]\n",
        "            if len(points) > 10:\n",
        "                hull = cv2.convexHull(points)\n",
        "                cv2.fillConvexPoly(mask, hull, 255)\n",
        "                face_found = True\n",
        "\n",
        "    # Fallback: use face detection if mesh failed\n",
        "    if not face_found:\n",
        "        results_det = face_detection.process(rgb)\n",
        "        if results_det.detections:\n",
        "            for det in results_det.detections:\n",
        "                bbox = det.location_data.relative_bounding_box\n",
        "                x1 = int(bbox.xmin * width)\n",
        "                y1 = int(bbox.ymin * height)\n",
        "                x2 = int((bbox.xmin + bbox.width) * width)\n",
        "                y2 = int((bbox.ymin + bbox.height) * height)\n",
        "                x1, y1 = max(0, x1), max(0, y1)\n",
        "                x2, y2 = min(width - 1, x2), min(height - 1, y2)\n",
        "                mask[y1:y2, x1:x2] = 255\n",
        "\n",
        "    blurred = cv2.GaussianBlur(frame, (99, 99), 30)\n",
        "    mask_3d = cv2.merge([mask, mask, mask])\n",
        "    frame_blurred = np.where(mask_3d == 255, blurred, frame)\n",
        "    out.write(frame_blurred.astype(np.uint8))\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "face_mesh.close()\n",
        "face_detection.close()\n",
        "\n",
        "print(\"Processing complete!\")\n",
        "files.download(output_video_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfuOLoqYjJcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}